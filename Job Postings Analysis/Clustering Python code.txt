# K-Means on job descriptions — sweep K=2..10, pick best; skip empties; auto-download Excel
!pip -q install pandas numpy scikit-learn openpyxl

import io, os, re, unicodedata, math, warnings
from datetime import datetime
import numpy as np
import pandas as pd

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import normalize
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples, calinski_harabasz_score, davies_bouldin_score
from scipy.sparse import hstack, issparse

warnings.filterwarnings("ignore")

# ====== CONFIG ======
K_RANGE = range(2, 11)
SVD_COMPONENTS = 200
MIN_DF, MAX_DF = 2, 0.8
WORD_MAX_FEATURES = 15000
CHAR_MAX_FEATURES = 8000
RANDOM_STATE = 42
DOMAIN_STOPWORDS = {
    "prompt","prompts","engineer","engineering","llm","llms","gpt","chatgpt","openai",
    "ai","artificial","intelligence","role","job","description","candidate","position",
    "experience","responsibilities","requirements","skills","team","work","company",
    "application","applications","models","model","llama","mistral","gemini","developer",
    "development","software","system","systems","data"
}
# ====================

URL_RE = re.compile(r'https?://\S+|www\.\S+', re.IGNORECASE)
EMAIL_RE = re.compile(r'\b[\w\.-]+@[\w\.-]+\.\w+\b', re.IGNORECASE)
CODE_BLOCK_RE = re.compile(r'`{1,3}[\s\S]*?`{1,3}', re.MULTILINE)
NON_ALPHA_RE = re.compile(r'[^a-zA-Z\s-]+')
WS_RE = re.compile(r'\s+')

def clean_text(s: str) -> str:
    if not isinstance(s, str):
        return ""
    s = unicodedata.normalize("NFKC", s).lower()
    s = URL_RE.sub(" ", s)
    s = EMAIL_RE.sub(" ", s)
    s = CODE_BLOCK_RE.sub(" ", s)
    s = NON_ALPHA_RE.sub(" ", s)
    s = WS_RE.sub(" ", s).strip()
    return s

def detect_text_column(df: pd.DataFrame) -> str:
    if df.shape[1] == 1:
        return df.columns[0]
    cand = [c for c in df.columns if df[c].dtype == object] or df.columns.tolist()
    avg_len = {c: df[c].astype(str).str.len().mean() for c in cand}
    return max(avg_len, key=avg_len.get)

def noun_phrase_candidates(text: str, max_len=5):
    tokens = [t for t in re.findall(r'[a-z]+(?:-[a-z]+)?', text)]
    cands = []
    for n in range(1, min(max_len, len(tokens)) + 1):
        for i in range(len(tokens)-n+1):
            phrase = " ".join(tokens[i:i+n])
            if len(phrase) >= 4:
                cands.append(phrase)
    return cands

def top_terms_from_sparse_mean(mean_vec, feature_names, topn=15):
    if issparse(mean_vec):
        mean_vec = mean_vec.toarray().ravel()
    else:
        mean_vec = np.asarray(mean_vec).ravel()
    if mean_vec.size == 0:
        return []
    idx = np.argsort(-mean_vec)[:min(topn, mean_vec.size)]
    out = []
    for i in idx.astype(int):
        if i < len(feature_names) and float(mean_vec[i]) > 0:
            out.append(feature_names[int(i)])
    return out

def pick_label_and_meaning(cluster_id, idxs, top_terms, texts_clean, texts_original):
    from collections import Counter
    candidates = Counter()
    for t in top_terms[:10]:
        if t not in DOMAIN_STOPWORDS:
            candidates[t] += 5
    for i in idxs:
        for phrase in noun_phrase_candidates(texts_clean.iloc[i]):
            if phrase in DOMAIN_STOPWORDS:
                continue
            candidates[phrase] += 1
    if candidates:
        best = max(candidates.items(), key=lambda x: x[1])[0]
        best = re.sub(r'\b(engineering|engineer|prompt|prompts|ai|llm|llms|gpt)\b', '', best).strip()
        best = re.sub(r'\s{2,}', ' ', best)
        label = " ".join(best.split()[:6]) if best and len(best) >= 3 else f"Cluster {cluster_id}"
    else:
        label = f"Cluster {cluster_id}"
    snippet = " ".join(texts_original.iloc[idxs[0]].split()[:28]) if idxs else ""
    meaning = (
        f"This cluster focuses on {label}, frequently mentioning: "
        f"{', '.join(top_terms[:6])}. Example: “{snippet}…”"
    )
    return label, meaning

def safe_metric(fn, *args, **kwargs):
    try:
        return fn(*args, **kwargs)
    except Exception:
        return np.nan

# --- Upload file (CSV/XLSX/JSON) ---
from google.colab import files
uploaded = files.upload()
if not uploaded:
    raise RuntimeError("No file uploaded.")
name = list(uploaded.keys())[0]
ext = os.path.splitext(name)[1].lower()
buffer = io.BytesIO(uploaded[name])

if ext == ".csv":
    df = pd.read_csv(buffer)
elif ext in (".xlsx", ".xls"):
    df = pd.read_excel(buffer)
elif ext == ".json":
    df = pd.read_json(buffer, lines=False)
else:
    raise ValueError(f"Unsupported file type: {ext}. Use CSV/XLSX/JSON.")

print(f"Loaded: {name}  shape={df.shape}")

# --- Filter empties BEFORE clustering; preserve original indices ---
text_col = detect_text_column(df)
col_series = df[text_col].fillna("")
texts_original_all = col_series.astype(str)
texts_clean_all = texts_original_all.apply(clean_text)

nonempty_mask = texts_clean_all.str.len() > 0
ignored_mask = ~nonempty_mask
orig_index = np.arange(len(df))

# Subset non-empty
texts_original = texts_original_all[nonempty_mask].reset_index(drop=False)
texts_clean    = texts_clean_all[nonempty_mask].reset_index(drop=False)
kept_orig_idx  = texts_original["index"].to_numpy()
texts_original = texts_original[text_col]
texts_clean    = texts_clean[text_col]

print(f"Using column: `{text_col}` | Kept {len(texts_clean)} non-empty rows | Ignored {ignored_mask.sum()} empty rows.")

# --- TF-IDF (word + char), then SVD ---
word_vec = TfidfVectorizer(
    ngram_range=(1,2), max_features=WORD_MAX_FEATURES,
    min_df=MIN_DF, max_df=MAX_DF, stop_words="english"
)
char_vec = TfidfVectorizer(
    analyzer="char", ngram_range=(3,5), max_features=CHAR_MAX_FEATURES,
    min_df=MIN_DF, max_df=MAX_DF
)

Xw = word_vec.fit_transform(texts_clean)
Xc = char_vec.fit_transform(texts_clean)
X  = hstack([Xw, Xc]).tocsr()
feature_names = np.concatenate([word_vec.get_feature_names_out(), char_vec.get_feature_names_out()])
print(f"TF-IDF shape: {X.shape}")

svd = TruncatedSVD(n_components=min(SVD_COMPONENTS, X.shape[1]-1), random_state=RANDOM_STATE)
X_svd = svd.fit_transform(X)
X_emb = normalize(X_svd, norm="l2")

# Robust print: support either n_components_ or n_components across sklearn versions
ncomp = getattr(svd, "n_components_", None)
if ncomp is None:
    ncomp = getattr(svd, "n_components", None)
print(f"SVD components: {ncomp} | cumulative explained variance: {svd.explained_variance_ratio_.sum():.3f}")

# --- Sweep K = 2..10, score, and select best ---
results = []
best = None  # (score tuple, K, model, labels)

for K in K_RANGE:
    km = KMeans(n_clusters=K, n_init=10, max_iter=300, random_state=RANDOM_STATE)
    labels = km.fit_predict(X_emb)

    sil = safe_metric(silhouette_score, X_emb, labels, metric="cosine") if len(np.unique(labels)) > 1 else np.nan
    ch  = safe_metric(calinski_harabasz_score, X_emb, labels)
    db  = safe_metric(davies_bouldin_score, X_emb, labels)

    results.append({"K": K, "silhouette_cosine": sil, "calinski_harabasz": ch, "davies_bouldin": db})

    # Selection key: higher Silhouette, then higher CH, then lower DB
    key = (float(sil) if not np.isnan(sil) else -np.inf,
           float(ch)  if not np.isnan(ch)  else -np.inf,
           -float(db) if not np.isnan(db) else -np.inf)

    if (best is None) or (key > best[0]):
        best = (key, K, km, labels)

# Report model selection
model_selection = pd.DataFrame(results).sort_values(
    by=["silhouette_cosine","calinski_harabasz","davies_bouldin"],
    ascending=[False, False, True]
).reset_index(drop=True)

best_key, BEST_K, best_km, best_labels = best
best_row = model_selection.iloc[0]
print("\n==================== MODEL SELECTION (K=2..10) ====================")
print(model_selection.to_string(index=False))
print(f"\nSelected K = {BEST_K}  "
      f"(Silhouette={best_row['silhouette_cosine']:.3f}, "
      f"CH={best_row['calinski_harabasz']:.2f}, "
      f"DB={best_row['davies_bouldin']:.3f})")

# --- Per-sample silhouettes & cluster summaries for BEST_K ---
sil_samples = safe_metric(silhouette_samples, X_emb, best_labels, metric="cosine")
if isinstance(sil_samples, float) and np.isnan(sil_samples):
    sil_samples = np.full(len(texts_clean), np.nan)

cluster_indices = {c: np.where(best_labels == c)[0].tolist() for c in np.unique(best_labels)}
cluster_top_terms, cluster_labels_map, cluster_meanings_map, cluster_avg_sil = {}, {}, {}, {}

for c, idxs in cluster_indices.items():
    cm = X[idxs].mean(axis=0) if len(idxs) else None
    cluster_top_terms[c] = top_terms_from_sparse_mean(cm, feature_names, topn=20) if cm is not None else []
    label, meaning = pick_label_and_meaning(c, idxs, cluster_top_terms[c], texts_clean, texts_original)
    cluster_labels_map[c] = label
    cluster_meanings_map[c] = meaning
    cluster_avg_sil[c] = float(np.nanmean(sil_samples[idxs])) if len(idxs) and not np.isnan(sil_samples[idxs]).all() else np.nan

print("\n==================== BEST MODEL CLUSTERS ====================")
print(f"Best K = {BEST_K}")
for c in sorted(cluster_indices.keys()):
    size = len(cluster_indices[c])
    label = cluster_labels_map[c]
    meaning = cluster_meanings_map[c]
    avg_sil = cluster_avg_sil[c]
    terms = ", ".join(cluster_top_terms[c][:10])
    example = " ".join(texts_original.iloc[cluster_indices[c][0]].split()[:28]) if size else ""
    print(f"\nCluster {c}  (size={size}, avg_silhouette={avg_sil:.3f} if available)")
    print(f"Name   : {label}")
    print(f"Meaning: {meaning}")
    print(f"Top terms: {terms}")
    if example:
        print(f"Example: {example}…")

# --- Excel outputs (non-empty mapping + summary + model_selection + ignored) ---
data_with_clusters = pd.DataFrame({
    "original_row_id": kept_orig_idx,
    "original_text": texts_original.values,
    "cleaned_text": texts_clean.values,
    "best_k": BEST_K,
    "kmeans_cluster_id": best_labels,
    "kmeans_cluster_label": [cluster_labels_map[c] for c in best_labels],
    "kmeans_cluster_meaning": [cluster_meanings_map[c].split(".")[0] + "." for c in best_labels],
    "silhouette_sample": sil_samples
})

ignored_rows = pd.DataFrame({
    "original_row_id": np.arange(len(df))[ignored_mask],
    "original_text": texts_original_all[ignored_mask].values
}).reset_index(drop=True)

summary_rows = []
for c in sorted(cluster_indices.keys()):
    summary_rows.append({
        "cluster_id": c,
        "size": len(cluster_indices[c]),
        "label": cluster_labels_map[c],
        "meaning": cluster_meanings_map[c],
        "avg_silhouette": cluster_avg_sil[c],
        "top_terms": ", ".join(cluster_top_terms[c][:12]),
    })
cluster_summary = pd.DataFrame(summary_rows)

stamp = datetime.now().strftime("%Y%m%d_%H%M")
xlsx_path = f"prompt_engineer_kmeans_bestK_{stamp}.xlsx"
with pd.ExcelWriter(xlsx_path, engine="openpyxl") as writer:
    data_with_clusters.to_excel(writer, index=False, sheet_name="data_with_clusters")
    cluster_summary.to_excel(writer, index=False, sheet_name="cluster_summary_best")
    model_selection.to_excel(writer, index=False, sheet_name="model_selection")
    ignored_rows.to_excel(writer, index=False, sheet_name="ignored_rows")

print(f"\nExcel saved to: {os.path.abspath(xlsx_path)}")

from google.colab import files as colab_files
colab_files.download(xlsx_path)
